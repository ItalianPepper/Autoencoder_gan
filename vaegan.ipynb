{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vaegan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItalianPepper/VAE-GAN/blob/master/vaegan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvY5gNnwN0p"
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from keras import metrics, backend as K\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from random import shuffle\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers import *\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset(path, img_shape):\n",
        "    paths = []\n",
        "\n",
        "    valid_format = [\".jpg\", \".png\", \".tiff\", \".bmp\", \".gif\"]\n",
        "\n",
        "    for img in os.listdir(path):\n",
        "\n",
        "        extension = os.path.splitext(img)[1]\n",
        "\n",
        "        if extension.lower() in valid_format:\n",
        "            paths.append(os.path.join(path, img))\n",
        "\n",
        "    k_train = []\n",
        "\n",
        "    for path_image in paths:\n",
        "        img = load_img(path_image, target_size=img_shape[:2])\n",
        "        \n",
        "        el = img_to_array(img) / 255.0\n",
        "        \n",
        "        k_train.append(el)\n",
        "\n",
        "    k_train = np.asarray(k_train)\n",
        "\n",
        "    return k_train\n",
        "\n",
        "  \n",
        "def get_noise(n_sample=1, nlatent_dim=512):\n",
        "    noise = np.random.normal(0, 1, (n_sample, nlatent_dim))\n",
        "    return (noise)\n",
        "  \n",
        "\n",
        "def plot_generated_images(noise, nsample=1, path_save=None, epoch=0):\n",
        "    imgs = decoder.predict(noise)\n",
        "    img = imgs[0]\n",
        "    \n",
        "    # Solution: \"Clipping input data to the valid range for imshow \n",
        "    # with RGB data ([0..1] for floats or [0..255] for integers).\"\n",
        "    \n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    \n",
        "    fig = plt.figure(figsize=(40, 10))\n",
        "    \n",
        "    epoch_str = str(epoch)\n",
        "    extension = \".png\"\n",
        "    path_name = path_save + \"/\" + epoch_str + \" \" + extension\n",
        "    \n",
        "    ax = fig.add_subplot(1, nsample, 1)\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    fig.suptitle(\"Epoch:\" + epoch_str, fontsize=30)\n",
        "\n",
        "    plt.savefig(path_name,\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\"\"\"\n",
        "def sampling(args):\n",
        "    mean, logsigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(mean)[0], 512), mean=0., stddev=1.0)\n",
        "    return mean + K.exp(logsigma / 2) * epsilon\n",
        "\"\"\"\n",
        "\n",
        "def build_encoder(img_shape):\n",
        "    input_img = Input(shape=img_shape)\n",
        "    n_filter = 32\n",
        "    \n",
        "    # Encoding\n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(input_img)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*8, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    #mean = Dense(512)(x)\n",
        "    logsigma = Dense(512, activation=\"tanh\")(x)\n",
        "    #latent = Lambda(sampling, output_shape=(512,))([mean, logsigma])\n",
        "    \n",
        "    model = Model(inputs=input_img, outputs=logsigma)\n",
        "    model.summary()\n",
        "    return (model)\n",
        "  \n",
        "def build_decoder_gen(en_shape=(512,)):\n",
        "    # impostazione del generatore\n",
        "    en_img = Input(shape=en_shape)\n",
        "    img_dim = 32 * 16\n",
        "    n_filter = 32\n",
        "    \n",
        "    x = Dense(img_dim*8*8)(en_img)\n",
        "    x = Reshape((8,8,img_dim))(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    # x = Activation(\"relu\")(x)\n",
        "    \n",
        "    # Decoding\n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    #x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D()(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*2, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    #x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D()(x)\n",
        "    \n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    #x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = Activation('tanh')(x)\n",
        "    \n",
        "    model = Model(inputs=en_img, outputs=x)\n",
        "    model.summary()\n",
        "    return (model)\n",
        "  \n",
        "def build_discriminator(img_shape):\n",
        "  \n",
        "    input_img = Input(shape=img_shape)\n",
        "    n_filter = 32\n",
        "    \n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding='same', strides=2)(input_img)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*2, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(128, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    \n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=input_img, outputs=out)\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def train(encoder, decoder, discriminator, gan, x_train,\n",
        "          dir_result=\"./\", epochs=10000, batch_size=512):\n",
        "   \n",
        "    history = []\n",
        "    \n",
        "    if epochs >= 100:\n",
        "      checkpoint = int(epochs/100)\n",
        "    else:\n",
        "      checkpoint = 1\n",
        "    \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "        start_epoch = time.time()\n",
        "        \n",
        "        np.random.shuffle(x_train)\n",
        "        \n",
        "        batches_index = [x for x in range(0, len(x_train), batch_size)]\n",
        "        \n",
        "        for i in range(0, len(batches_index)):\n",
        "          \n",
        "          start = batches_index[i]\n",
        "          \n",
        "          if i == len(batches_index)-1:\n",
        "            end = len(x_train)\n",
        "          else:\n",
        "            end = batches_index[i+1]\n",
        "            \n",
        "          x_batch = x_train[start:end]\n",
        "          x_batch_size = len(x_batch)\n",
        "          \n",
        "          \n",
        "          # encoder e decoder su immagini reali\n",
        "          gen_img_enc = encoder.predict(x_batch)\n",
        "          gen_img_dec = decoder.predict(gen_img_enc)\n",
        "\n",
        "          noise = get_noise(n_sample=x_batch_size)\n",
        "          # immagini create dal noise\n",
        "          fake_img = decoder.predict(noise)\n",
        "\n",
        "          matrix_true = np.ones((x_batch_size, 1))\n",
        "          matrix_false = np.zeros((x_batch_size, 1))\n",
        "          \n",
        "          # Allenamento discriminatore\n",
        "          dsc_true = discriminator.train_on_batch(x_batch, matrix_true)\n",
        "          dsc_true_enc = discriminator.train_on_batch(gen_img_dec, matrix_true)\n",
        "          dsc_fake = discriminator.train_on_batch(fake_img, matrix_false)\n",
        "\n",
        "          # Allenamento decoder\n",
        "          \n",
        "          noise_gen = get_noise(n_sample=x_batch_size)\n",
        "          decoder_noise_loss = decoder.train_on_batch(noise_gen, x_batch)\n",
        "          \n",
        "          #decoder_enc_loss = decoder.train_on_batch(gen_img_enc, x_batch)\n",
        "        \n",
        "          # dec_loss = 0.5 * np.add(decoder_enc_loss, decoder_noise_loss)\n",
        "\n",
        "          # Allenament GAN - Allena l'encoder\n",
        "          gan_loss = gan.train_on_batch(x_batch, matrix_true)\n",
        "\n",
        "        if epoch % checkpoint == 0 and epoch > 0:\n",
        "          history.append({\"Epoch:\": epoch,\n",
        "                          \"Discriminator_loss_true:\": dsc_true[0],\n",
        "                          \"Discriminator_loss_true_enc:\": dsc_true_enc[0],\n",
        "                          \"Discriminator_loss_fake\": dsc_fake[0],\n",
        "                          \"Discriminator_acc_fake\": dsc_fake[1],\n",
        "                          \"Decoder_noise_loss\": decoder_noise_loss,\n",
        "                          \"GAN_Loss:\": gan_loss})\n",
        "\n",
        "          noise = get_noise(n_sample=1)\n",
        "\n",
        "          plot_generated_images(noise, 1, dir_result, epoch)\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        end_epoch = end_epoch - start_epoch\n",
        "        print(\"Ended Epoch {:0.0f}/{:1.0f} in: {:2.4f} s\".format(epoch + 1, epochs, end_epoch))\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "#https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628\n",
        "#TTUR\n",
        "\n",
        "img_shape = (64, 64, 3)\n",
        "\n",
        "optimizer_std = Adam(0.0001)\n",
        "optimizer_disc = Adam(0.0004)\n",
        "\n",
        "\n",
        "encoder = build_encoder(img_shape)\n",
        "decoder = build_decoder_gen()\n",
        "discriminator = build_discriminator(img_shape)\n",
        "\n",
        "encoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "decoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "\n",
        "discriminator.compile(loss=\"binary_crossentropy\",\n",
        "                            optimizer=optimizer_disc,\n",
        "                            metrics=[\"accuracy\"])\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "x = Input(shape=img_shape)\n",
        "z = encoder(x)\n",
        "out_dec = decoder(z)\n",
        "out = discriminator(out_dec)\n",
        "\n",
        "gan = Model(inputs=x, outputs=out)\n",
        "\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer_std)\n",
        "\n",
        "x_train = get_dataset(\"./img_align_celeba\", img_shape)\n",
        "\n",
        "path = \"./drive/My Drive/GAN/res_gan_autoencoder/\"\n",
        "\n",
        "history = train(encoder, decoder, discriminator, gan, x_train,\n",
        "                dir_result=path, epochs=10000, batch_size=500)\n",
        "\n",
        "with open(path+\"log_gan.txt\", \"w\") as f:\n",
        "  for item in history:\n",
        "    item_str = str(item) + \"\\n\"\n",
        "    f.write(item_str)\n",
        "    \n",
        "discriminator.trainable = True\n",
        "\n",
        "encoder.save(path+\"encoder.h5\")\n",
        "decoder.save(path+\"decoder.h5\")\n",
        "discriminator.save(path+\"discriminator.h5\")\n",
        "gan.save(path+\"gan_autoencoder.h5\")\n",
        "\n",
        "end = time.time()\n",
        "end = end - start\n",
        "print(\"Finished in: \" + str(end) + \" s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzLNqwDumbSH"
      },
      "source": [
        "!unzip -Z1 \"/content/drive/My Drive/GAN/img_align_celeba.zip\" | head -501 | sed 's| |\\\\ |g' | xargs unzip \"/content/drive/My Drive/GAN/img_align_celeba.zip\" -d ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCdLT1QhtXV-"
      },
      "source": [
        "!mkdir ./res_gan_autoencoder"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4aBNjOjCqJr"
      },
      "source": [
        "!rm -r img_align_celeba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWlMvZnpMi-Y"
      },
      "source": [
        "!rm -r /content/res_gan_autoencoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIYDIXKvCYgo"
      },
      "source": [
        "!rm -r /content/img_align_celeba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRoJdYrbjTUT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0QzCyAxs2Dq"
      },
      "source": [
        "#Codice da utlizzare per allenare il modello dopo il limite di ore.\n",
        "#NON COMPILARE I MODELLI.\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from keras import metrics, backend as K\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from random import shuffle\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers import *\n",
        "\n",
        "\n",
        "def get_dataset(path, img_shape):\n",
        "    paths = []\n",
        "\n",
        "    valid_format = [\".jpg\", \".png\", \".tiff\", \".bmp\", \".gif\"]\n",
        "\n",
        "    for img in os.listdir(path):\n",
        "\n",
        "        extension = os.path.splitext(img)[1]\n",
        "\n",
        "        if extension.lower() in valid_format:\n",
        "            paths.append(os.path.join(path, img))\n",
        "\n",
        "    k_train = []\n",
        "\n",
        "    for path_image in paths:\n",
        "        img = load_img(path_image, target_size=img_shape[:2])\n",
        "        \n",
        "        el = img_to_array(img) / 255.0\n",
        "        \n",
        "        k_train.append(el)\n",
        "\n",
        "    k_train = np.asarray(k_train)\n",
        "\n",
        "    return k_train\n",
        "\n",
        "  \n",
        "def get_noise(n_sample=1, nlatent_dim=512):\n",
        "    noise = np.random.normal(0, 1, (n_sample, nlatent_dim))\n",
        "    return (noise)\n",
        "\n",
        "  \n",
        "def plot_generated_images(noise, nsample=1, path_save=None, epoch=0):\n",
        "    imgs = decoder.predict(noise)\n",
        "    #img_d = deblurring_resnet.predict(imgs)\n",
        "    img = imgs[0]\n",
        "    \n",
        "    # Solution: \"Clipping input data to the valid range for imshow \n",
        "    # with RGB data ([0..1] for floats or [0..255] for integers).\"\n",
        "    \n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    \n",
        "    fig = plt.figure(figsize=(40, 10))\n",
        "    \n",
        "    epoch_str = str(epoch)\n",
        "    extension = \".png\"\n",
        "    path_name = path_save + \"/\" + epoch_str + \" \" + extension\n",
        "    \n",
        "    ax = fig.add_subplot(1, nsample, 1)\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    fig.suptitle(\"Epoch:\" + epoch_str, fontsize=30)\n",
        "\n",
        "    plt.savefig(path_name,\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train(encoder, decoder, discriminator, gan, x_train,\n",
        "          dir_result=\"./\", epochs=10000, batch_size=512):\n",
        "   \n",
        "    history = []\n",
        "    \n",
        "    if epochs >= 100:\n",
        "      checkpoint = int(epochs/100)\n",
        "    else:\n",
        "      checkpoint = 1\n",
        "    \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "        start_epoch = time.time()\n",
        "        \n",
        "        np.random.shuffle(x_train)\n",
        "        \n",
        "        batches_index = [x for x in range(0, len(x_train), batch_size)]\n",
        "        \n",
        "        for i in range(0, len(batches_index)):\n",
        "          \n",
        "          start = batches_index[i]\n",
        "          \n",
        "          if i == len(batches_index)-1:\n",
        "            end = len(x_train)\n",
        "          else:\n",
        "            end = batches_index[i+1]\n",
        "            \n",
        "          x_batch = x_train[start:end]\n",
        "          x_batch_size = len(x_batch)\n",
        "          \n",
        "          \n",
        "          # encoder e decoder su immagini reali\n",
        "          gen_img_enc = encoder.predict(x_batch)\n",
        "          gen_img_dec = decoder.predict(gen_img_enc)\n",
        "\n",
        "          noise = get_noise(n_sample=x_batch_size)\n",
        "          # immagini create dal noise\n",
        "          fake_img = decoder.predict(noise)\n",
        "\n",
        "          matrix_true = np.ones((x_batch_size, 1))\n",
        "          matrix_false = np.zeros((x_batch_size, 1))\n",
        "          \n",
        "          # Allenamento discriminatore\n",
        "          dsc_true = discriminator.train_on_batch(x_batch, matrix_true)\n",
        "          dsc_true_enc = discriminator.train_on_batch(gen_img_dec, matrix_true)\n",
        "          dsc_fake = discriminator.train_on_batch(fake_img, matrix_false)\n",
        "\n",
        "          # Allenamento decoder\n",
        "          noise_gen = get_noise(n_sample=x_batch_size)\n",
        "          \n",
        "          decoder_noise_loss = decoder.train_on_batch(noise_gen, x_batch)\n",
        "\n",
        "          #dec_loss = 0.5 * np.add(decoder_enc_loss, decoder_noise_loss)\n",
        "\n",
        "          # Allenament GAN - Allena l'encoder\n",
        "          gan_loss = gan.train_on_batch(x_batch, matrix_true)\n",
        "\n",
        "        if epoch % checkpoint == 0 and epoch > 0:\n",
        "          history.append({\"Epoch:\": epoch,\n",
        "                          \"Discriminator_loss_true:\": dsc_true[0],\n",
        "                          \"Discriminator_loss_true_enc:\": dsc_true_enc[0],\n",
        "                          \"Discriminator_loss_fake\": dsc_fake[0],\n",
        "                          \"Discriminator_acc_fake\": dsc_fake[1],\n",
        "                          \"Decoder_loss\": decoder_noise_loss,\n",
        "                          \"GAN_Loss:\": gan_loss})\n",
        "\n",
        "          noise = get_noise(n_sample=1)\n",
        "\n",
        "          plot_generated_images(noise, 1, dir_result, epoch)\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        end_epoch = end_epoch - start_epoch\n",
        "        print(\"Ended Epoch {:0.0f}/{:1.0f} in: {:2.4f} s\".format(epoch + 1, epochs, end_epoch))\n",
        "\n",
        "    return history\n",
        "  \n",
        "\n",
        "start = time.time()\n",
        "\n",
        "img_shape = (64, 64, 3)\n",
        "optimizer = Adam(0.0001)\n",
        "\n",
        "path_load_models = \"./drive/My Drive/GAN/train_2_img_celeba/models/\"\n",
        "\n",
        "encoder = load_model(path_load_models+\"encoder.h5\")\n",
        "decoder = load_model(path_load_models+\"decoder.h5\")\n",
        "# https://github.com/keras-team/keras/issues/9589\n",
        "# gan = load_model(path_load_models+\"gan_autoencoder.h5\")\n",
        "discriminator = load_model(path_load_models+\"discriminator.h5\")\n",
        "# NON RICOMPILARE I MODELLI CARICATI.\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "x = Input(shape=img_shape)\n",
        "z = encoder(x)\n",
        "out_dec = decoder(z)\n",
        "out = discriminator(out_dec)\n",
        "\n",
        "gan = Model(inputs=x, outputs=out)\n",
        "\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "x_train = get_dataset(\"./img_align_celeba\", img_shape)\n",
        "\n",
        "path = \"./drive/My Drive/GAN/res_gan_autoencoder/\"\n",
        "\n",
        "history = train(encoder, decoder, discriminator, gan, x_train_slice, \n",
        "                dir_result=path, epochs=10000, batch_size=500)\n",
        "    \n",
        "with open(path+\"log_gan.txt\", \"w\") as f:\n",
        "    for item in history:\n",
        "      item_str = str(item) + \"\\n\"\n",
        "      f.write(item_str)\n",
        "      \n",
        "discriminator.trainable = True\n",
        "\n",
        "encoder.save(path+\"encoder.h5\")\n",
        "decoder.save(path+\"decoder.h5\")\n",
        "discriminator.save(path+\"discriminator.h5\")\n",
        "gan.save(path+\"gan_autoencoder.h5\")\n",
        "\n",
        "end = time.time()\n",
        "end = end - start\n",
        "print(\"Finished in: \" + str(end) + \" s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xJWSpDdX7Q-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from keras import metrics, backend as K\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from random import shuffle\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers import *\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset(path, img_shape):\n",
        "    paths = []\n",
        "\n",
        "    valid_format = [\".jpg\", \".png\", \".tiff\", \".bmp\", \".gif\"]\n",
        "\n",
        "    for img in os.listdir(path):\n",
        "\n",
        "        extension = os.path.splitext(img)[1]\n",
        "\n",
        "        if extension.lower() in valid_format:\n",
        "            paths.append(os.path.join(path, img))\n",
        "\n",
        "    k_train = []\n",
        "\n",
        "    for path_image in paths:\n",
        "        img = load_img(path_image, target_size=img_shape[:2])\n",
        "        \n",
        "        el = img_to_array(img) / 255.0\n",
        "        \n",
        "        k_train.append(el)\n",
        "\n",
        "    k_train = np.asarray(k_train)\n",
        "\n",
        "    return k_train\n",
        "\n",
        "  \n",
        "def get_noise(n_sample=1, nlatent_dim=512):\n",
        "    noise = np.random.normal(0, 1, (n_sample, nlatent_dim))\n",
        "    return (noise)\n",
        "  \n",
        "\n",
        "def plot_generated_images(noise, nsample=1, path_save=None, epoch=0):\n",
        "    imgs = decoder.predict(noise)\n",
        "    img = imgs[0]\n",
        "    \n",
        "    # Solution: \"Clipping input data to the valid range for imshow \n",
        "    # with RGB data ([0..1] for floats or [0..255] for integers).\"\n",
        "    \n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    \n",
        "    fig = plt.figure(figsize=(40, 10))\n",
        "    \n",
        "    epoch_str = str(epoch)\n",
        "    extension = \".png\"\n",
        "    path_name = path_save + \"/\" + epoch_str + \" \" + extension\n",
        "    \n",
        "    ax = fig.add_subplot(1, nsample, 1)\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    fig.suptitle(\"Epoch:\" + epoch_str, fontsize=30)\n",
        "\n",
        "    plt.savefig(path_name,\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\"\"\"\n",
        "def sampling(args):\n",
        "    mean, logsigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(mean)[0], 512), mean=0., stddev=1.0)\n",
        "    return mean + K.exp(logsigma / 2) * epsilon\n",
        "\"\"\"\n",
        "\n",
        "def build_encoder(img_shape):\n",
        "    input_img = Input(shape=img_shape)\n",
        "    n_filter = 32\n",
        "    \n",
        "    # Encoding\n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(input_img)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*8, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    #mean = Dense(512)(x)\n",
        "    logsigma = Dense(512, activation=\"tanh\")(x)\n",
        "    #latent = Lambda(sampling, output_shape=(512,))([mean, logsigma])\n",
        "    \n",
        "    model = Model(inputs=input_img, outputs=logsigma)\n",
        "    model.summary()\n",
        "    return (model)\n",
        "  \n",
        "def build_decoder_gen(en_shape=(512,)):\n",
        "    # impostazione del generatore\n",
        "    en_img = Input(shape=en_shape)\n",
        "    img_dim = 32 * 16\n",
        "    n_filter = 32\n",
        "    \n",
        "    x = Dense(img_dim*8*8)(en_img)\n",
        "    x = Reshape((8,8,img_dim))(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    # x = Activation(\"relu\")(x)\n",
        "    \n",
        "    # Decoding\n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    #x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D()(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*2, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    #x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D()(x)\n",
        "    \n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    #x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = Activation('tanh')(x)\n",
        "    \n",
        "    model = Model(inputs=en_img, outputs=x)\n",
        "    model.summary()\n",
        "    return (model)\n",
        "  \n",
        "def build_discriminator(img_shape):\n",
        "  \n",
        "    input_img = Input(shape=img_shape)\n",
        "    n_filter = 32\n",
        "    \n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding='same', strides=2)(input_img)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*2, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(128, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    \n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=input_img, outputs=out)\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def train(encoder, decoder, discriminator, gan, x_train,\n",
        "          dir_result=\"./\", epochs=10000, batch_size=512):\n",
        "   \n",
        "    history = []\n",
        "    \n",
        "    if epochs >= 100:\n",
        "      checkpoint = int(epochs/100)\n",
        "    else:\n",
        "      checkpoint = 1\n",
        "    \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "        start_epoch = time.time()\n",
        "        \n",
        "        np.random.shuffle(x_train)\n",
        "        \n",
        "        batches_index = [x for x in range(0, len(x_train), batch_size)]\n",
        "        \n",
        "        for i in range(0, len(batches_index)):\n",
        "          \n",
        "          start = batches_index[i]\n",
        "          \n",
        "          if i == len(batches_index)-1:\n",
        "            end = len(x_train)\n",
        "          else:\n",
        "            end = batches_index[i+1]\n",
        "            \n",
        "          x_batch = x_train[start:end]\n",
        "          x_batch_size = len(x_batch)\n",
        "          \n",
        "          \n",
        "          # encoder e decoder su immagini reali\n",
        "          gen_img_enc = encoder.predict(x_batch)\n",
        "          gen_img_dec = decoder.predict(gen_img_enc)\n",
        "\n",
        "          noise = get_noise(n_sample=x_batch_size)\n",
        "          # immagini create dal noise\n",
        "          fake_img = decoder.predict(noise)\n",
        "\n",
        "          matrix_true = np.ones((x_batch_size, 1))\n",
        "          matrix_false = np.zeros((x_batch_size, 1))\n",
        "          \n",
        "          # Allenamento discriminatore\n",
        "          dsc_true = discriminator.train_on_batch(x_batch, matrix_true)\n",
        "          dsc_true_enc = discriminator.train_on_batch(gen_img_dec, matrix_true)\n",
        "          dsc_fake = discriminator.train_on_batch(fake_img, matrix_false)\n",
        "\n",
        "          # Allenamento decoder\n",
        "          \n",
        "          noise_gen = get_noise(n_sample=x_batch_size)\n",
        "          decoder_noise_loss = decoder.train_on_batch(noise_gen, x_batch)\n",
        "          \n",
        "          #decoder_enc_loss = decoder.train_on_batch(gen_img_enc, x_batch)\n",
        "        \n",
        "          # dec_loss = 0.5 * np.add(decoder_enc_loss, decoder_noise_loss)\n",
        "\n",
        "          # Allenament GAN - Allena l'encoder\n",
        "          gan_loss = gan.train_on_batch(x_batch, matrix_true)\n",
        "\n",
        "        if epoch % checkpoint == 0 and epoch > 0:\n",
        "          history.append({\"Epoch:\": epoch,\n",
        "                          \"Discriminator_loss_true:\": dsc_true[0],\n",
        "                          \"Discriminator_loss_true_enc:\": dsc_true_enc[0],\n",
        "                          \"Discriminator_loss_fake\": dsc_fake[0],\n",
        "                          \"Discriminator_acc_fake\": dsc_fake[1],\n",
        "                          \"Decoder_noise_loss\": decoder_noise_loss,\n",
        "                          \"GAN_Loss:\": gan_loss})\n",
        "\n",
        "          noise = get_noise(n_sample=1)\n",
        "\n",
        "          plot_generated_images(noise, 1, dir_result, epoch)\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        end_epoch = end_epoch - start_epoch\n",
        "        print(\"Ended Epoch {:0.0f}/{:1.0f} in: {:2.4f} s\".format(epoch + 1, epochs, end_epoch))\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "#https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628\n",
        "#TTUR\n",
        "\n",
        "img_shape = (64, 64, 3)\n",
        "\n",
        "optimizer_std = Adam(0.0001)\n",
        "optimizer_disc = Adam(0.0004)\n",
        "\n",
        "\n",
        "encoder = build_encoder(img_shape)\n",
        "decoder = build_decoder_gen()\n",
        "discriminator = build_discriminator(img_shape)\n",
        "\n",
        "encoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "decoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "\n",
        "discriminator.compile(loss=\"binary_crossentropy\",\n",
        "                            optimizer=optimizer_disc,\n",
        "                            metrics=[\"accuracy\"])\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "x = Input(shape=img_shape)\n",
        "z = encoder(x)\n",
        "out_dec = decoder(z)\n",
        "out = discriminator(out_dec)\n",
        "\n",
        "gan = Model(inputs=x, outputs=out)\n",
        "\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer_std)\n",
        "\n",
        "x_train = get_dataset(\"./img_align_celeba\", img_shape)\n",
        "\n",
        "path = \"./drive/My Drive/GAN/res_gan_autoencoder/\"\n",
        "\n",
        "history = train(encoder, decoder, discriminator, gan, x_train,\n",
        "                dir_result=path, epochs=10000, batch_size=500)\n",
        "\n",
        "with open(path+\"log_gan.txt\", \"w\") as f:\n",
        "  for item in history:\n",
        "    item_str = str(item) + \"\\n\"\n",
        "    f.write(item_str)\n",
        "    \n",
        "discriminator.trainable = True\n",
        "\n",
        "encoder.save(path+\"encoder.h5\")\n",
        "decoder.save(path+\"decoder.h5\")\n",
        "discriminator.save(path+\"discriminator.h5\")\n",
        "gan.save(path+\"gan_autoencoder.h5\")\n",
        "\n",
        "end = time.time()\n",
        "end = end - start\n",
        "print(\"Finished in: \" + str(end) + \" s\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}