{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "easy_vaegan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItalianPepper/VAE-GAN/blob/master/easy_vaegan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cShMCBv6XnVd"
      },
      "source": [
        "# Some reference\n",
        "\n",
        "- VAEGAN original : https://arxiv.org/pdf/1512.09300.pdf\n",
        "- VAEGAN implementation https://github.com/crmaximo/VAEGAN\n",
        "- Inspiring during implementation : https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628\n",
        "- Linneaus 5 (dataset): http://chaladze.com/l5/\n",
        "- Keras problem with trainable: https://github.com/keras-team/keras/issues/9589"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRoJdYrbjTUT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzLNqwDumbSH"
      },
      "source": [
        "# Unzip a limited number of file\n",
        "#!unzip -Z1 \"*link\" | head -10001 | sed 's| |\\\\ |g' | xargs unzip \"*link\" -d ./\n",
        "# Linnaeus is .rar -> extract manually and transformed in .zip\n",
        "!unzip \"/content/drive/My Drive/Linnaeus 5 64X64.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCdLT1QhtXV-"
      },
      "source": [
        "!mkdir \"/content/drive/My Drive/result\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvY5gNnwN0p"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import metrics, backend as K\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "\n",
        "def get_dataset(path, img_shape):\n",
        "    paths = []\n",
        "\n",
        "    valid_format = [\".jpg\", \".png\", \".tiff\", \".bmp\", \".gif\"]\n",
        "\n",
        "    for img in os.listdir(path):\n",
        "\n",
        "        extension = os.path.splitext(img)[1]\n",
        "\n",
        "        if extension.lower() in valid_format:\n",
        "            paths.append(os.path.join(path, img))\n",
        "\n",
        "    k_train = []\n",
        "\n",
        "    for path_image in paths:\n",
        "        img = load_img(path_image, target_size=img_shape[:2])\n",
        "        \n",
        "        el = img_to_array(img) / 255.0\n",
        "        \n",
        "        k_train.append(el)\n",
        "\n",
        "    k_train = np.asarray(k_train)\n",
        "\n",
        "    return k_train\n",
        "\n",
        "  \n",
        "def get_noise(n_sample=1, nlatent_dim=512):\n",
        "    noise = np.random.normal(0, 1, (n_sample, nlatent_dim))\n",
        "    return (noise)\n",
        "  \n",
        "\n",
        "def plot_generated_images(noise, nsample=1, path_save=None, epoch=0):\n",
        "    imgs = decoder.predict(noise)\n",
        "    img = imgs[0]\n",
        "    \n",
        "    # Solution: \"Clipping input data to the valid range for imshow \n",
        "    # with RGB data ([0..1] for floats or [0..255] for integers).\"\n",
        "    \n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    \n",
        "    fig = plt.figure(figsize=(40, 10))\n",
        "    fig.patch.set_visible(False)\n",
        "    epoch_str = str(epoch)\n",
        "    extension = \".png\"\n",
        "    path_name = path_save + \"/\" + epoch_str + extension\n",
        "    \n",
        "    ax = fig.add_subplot(1, nsample, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    \n",
        "    #fig.suptitle(\"Epoch:\" + epoch_str, fontsize=30)\n",
        "    \n",
        "    plt.savefig(path_name,\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0)\n",
        "    \n",
        "    plt.close()\n",
        "\n",
        "def build_encoder(img_shape):\n",
        "    input_img = Input(shape=img_shape, name=\"input_encoder\")\n",
        "    n_filter = 32\n",
        "    \n",
        "    # Encoding\n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(input_img)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*8, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    logsigma = Dense(512, activation=\"tanh\")(x)\n",
        "    \n",
        "    model = Model(inputs=input_img, outputs=logsigma)\n",
        "    # model.summary()\n",
        "    return (model)\n",
        "  \n",
        "def build_decoder_gen(en_shape=(512,)):\n",
        "    # impostazione del generatore\n",
        "    en_img = Input(shape=en_shape, name=\"input_decoder\")\n",
        "    img_dim = 32 * 16\n",
        "    n_filter = 32\n",
        "    \n",
        "    x = Dense(img_dim*8*8)(en_img)\n",
        "    x = Reshape((8,8,img_dim))(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    # Decoding\n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = UpSampling2D()(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*2, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = UpSampling2D()(x)\n",
        "    \n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = UpSampling2D()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = Activation('tanh')(x)\n",
        "    \n",
        "    model = Model(inputs=en_img, outputs=x)\n",
        "    return (model)\n",
        "  \n",
        "def build_discriminator(img_shape):\n",
        "  \n",
        "    input_img = Input(shape=img_shape, name=\"input_discriminator\")\n",
        "    n_filter = 32\n",
        "    \n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding='same', strides=2)(input_img)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*2, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(n_filter*4, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    \n",
        "    x = Conv2D(128, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    \n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=input_img, outputs=out)\n",
        "\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def train(encoder, decoder, discriminator, gan, x_train,\n",
        "          dir_result=\"./\", epochs=10000, batch_size=512):\n",
        "   \n",
        "    history = []\n",
        "    \n",
        "    if epochs >= 100:\n",
        "      checkpoint = int(epochs/10)\n",
        "    else:\n",
        "      checkpoint = 1\n",
        "    \n",
        "\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "        start_epoch = time.time()\n",
        "        \n",
        "        np.random.shuffle(x_train)\n",
        "        \n",
        "        batches_index = [x for x in range(0, len(x_train), batch_size)]\n",
        "        \n",
        "        for i in range(0, len(batches_index)):\n",
        "          \n",
        "          start = batches_index[i]\n",
        "          \n",
        "          if i == len(batches_index)-1:\n",
        "            end = len(x_train)\n",
        "          else:\n",
        "            end = batches_index[i+1]\n",
        "            \n",
        "          x_batch = x_train[start:end]\n",
        "          x_batch_size = len(x_batch)\n",
        "          \n",
        "          matrix_true = np.ones((x_batch_size, 1))\n",
        "          matrix_false = np.zeros((x_batch_size, 1))\n",
        "\n",
        "          # encoder e decoder su immagini reali\n",
        "          gen_img_enc = encoder.predict(x_batch)\n",
        "          gen_img_dec = decoder.predict(gen_img_enc)\n",
        "\n",
        "          noise = get_noise(n_sample=x_batch_size)\n",
        "          # immagini create dal noise\n",
        "          fake_img = decoder.predict(noise)\n",
        "\n",
        "          # Allenamento decoder su noise\n",
        "          noise_gen = get_noise(n_sample=x_batch_size)\n",
        "          decoder_noise_loss = decoder.train_on_batch(noise_gen, x_batch)\n",
        "          \n",
        "          # Allenamento discriminatore\n",
        "          dsc_true = discriminator.train_on_batch(x_batch, matrix_true)\n",
        "          #dsc_gen = discriminator.train_on_batch(gen_img_dec, matrix_true)\n",
        "          dsc_fake = discriminator.train_on_batch(fake_img, matrix_false)\n",
        "          \n",
        "          #Allenamento encoder e stato della gan\n",
        "          discriminator.trainable = False\n",
        "          gan_loss = gan.train_on_batch(x_batch, matrix_true)\n",
        "          discriminator.trainable = True\n",
        "\n",
        "        if (epoch+1) % checkpoint == 0:\n",
        "          history.append({\"Epoch:\": epoch + 1,\n",
        "                          \"Discriminator_loss_true:\": dsc_true[0],\n",
        "                          \"Discriminator_loss_fake:\": dsc_fake[0],\n",
        "                          \"Discriminator_acc_fake:\": dsc_fake[1],\n",
        "                          \"Decoder_noise_loss:\": decoder_noise_loss,\n",
        "                          \"GAN_loss:\":gan_loss\n",
        "                          })\n",
        "\n",
        "          #noise = get_noise(n_sample=1)\n",
        "\n",
        "          #plot_generated_images(noise, 1, dir_result, epoch + 1)\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        end_epoch = end_epoch - start_epoch\n",
        "        print(\"Ended Epoch {:0.0f}/{:1.0f} in: {:2.4f} s\".format(epoch + 1, epochs, end_epoch))\n",
        "\n",
        "    return history\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "img_shape = (64, 64, 3)\n",
        "noise_shape = (512,)\n",
        "optimizer_std = SGD(0.0003)\n",
        "optimizer_disc = SGD(0.0005)\n",
        "\n",
        "pre_trained = False\n",
        "path_load_models = \"./drive/My Drive/GAN/\"\n",
        "\n",
        "if pre_trained == False:\n",
        "\n",
        "  encoder = build_encoder(img_shape)\n",
        "  decoder = build_decoder_gen(noise_shape)\n",
        "  discriminator = build_discriminator(img_shape)\n",
        "\n",
        "  encoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "  decoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "\n",
        "  discriminator.compile(loss=\"mse\", optimizer=optimizer_disc,\n",
        "                              metrics=[\"accuracy\"])\n",
        "\n",
        "  x_1 = Input(shape=img_shape)\n",
        "\n",
        "  encoded_img = encoder(x_1)\n",
        "  decoded_img = decoder(encoded_img)\n",
        "\n",
        "  img_val = discriminator(x_1)\n",
        "  encdec_val = discriminator(decoded_img)\n",
        "\n",
        "  gan = Model(inputs=[x_1], outputs=[img_val, encdec_val])\n",
        "\n",
        "  gan.compile(loss=\"mae\", optimizer=optimizer_std)\n",
        "\n",
        "else:\n",
        "  encoder = load_model(path_load_models+\"encoder.h5\")\n",
        "  decoder = load_model(path_load_models+\"decoder.h5\")\n",
        "  discriminator = load_model(path_load_models+\"discriminator.h5\")\n",
        "  gan = load_model(path_load_models+ \"gan.h5\")\n",
        "  \n",
        "\n",
        "x_train = get_dataset(\"/content/Linnaeus 5 64X64/train/flower\", img_shape)\n",
        "\n",
        "path = \"/content/drive/My Drive/result/\"\n",
        "\n",
        "history = train(encoder, decoder, discriminator, gan, x_train,\n",
        "                dir_result=path, epochs=10, batch_size=128)\n",
        "\n",
        "with open(path+\"log_gan.txt\", \"w\") as f:\n",
        "  for item in history:\n",
        "    item_str = str(item) + \"\\n\"\n",
        "    f.write(item_str)\n",
        "\n",
        "# test post training\n",
        "for i in range(0, 10):\n",
        "  noise = get_noise(n_sample=1)\n",
        "\n",
        "  plot_generated_images(noise, 1, path, epoch + 1)\n",
        "\n",
        "\"\"\"    \n",
        "encoder.save(path+\"encoder.h5\", save_format=\"tf\")\n",
        "decoder.save(path+\"decoder.h5\", save_format=\"tf\")\n",
        "discriminator.save(path+\"discriminator.h5\", save_format=\"tf\")\n",
        "gan.save(path+\"gan.h5\", save_format=\"tf\")\n",
        "\"\"\"\n",
        "\n",
        "end = time.time()\n",
        "end = end - start\n",
        "print(\"Finished in: \" + str(end) + \" s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWlMvZnpMi-Y"
      },
      "source": [
        "!rm -r ./result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCPFjesWPBAa"
      },
      "source": [
        "!rm -r ./Linnaeus\\ 5\\ 64X64"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}