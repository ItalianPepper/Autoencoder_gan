{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "easy_vaegan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItalianPepper/VAE-GAN/blob/master/easy_vaegan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cShMCBv6XnVd"
      },
      "source": [
        "# Some reference\n",
        "\n",
        "- VAEGAN original : https://arxiv.org/pdf/1512.09300.pdf\n",
        "- VAEGAN implementation https://github.com/crmaximo/VAEGAN\n",
        "- Inspiring during implementation : https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628\n",
        "- Linneaus 5 (dataset): http://chaladze.com/l5/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRoJdYrbjTUT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzLNqwDumbSH"
      },
      "source": [
        "# Unzip a limited number of file\n",
        "#!unzip -Z1 \"*link\" | head -10001 | sed 's| |\\\\ |g' | xargs unzip \"*link\" -d ./\n",
        "# Linnaeus is .rar -> extract manually and transformed in .zip\n",
        "!unzip \"/content/drive/My Drive/Linnaeus 5 64X64.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCdLT1QhtXV-"
      },
      "source": [
        "!mkdir \"/content/drive/My Drive/result\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5gTFC-5QLOp"
      },
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "\n",
        "def get_dataset(path, img_shape):\n",
        "    paths = []\n",
        "\n",
        "    valid_format = [\".jpg\", \".png\", \".tiff\", \".bmp\", \".gif\"]\n",
        "\n",
        "    for img in os.listdir(path):\n",
        "\n",
        "        extension = os.path.splitext(img)[1]\n",
        "\n",
        "        if extension.lower() in valid_format:\n",
        "            paths.append(os.path.join(path, img))\n",
        "\n",
        "    k_train = []\n",
        "\n",
        "    for path_image in paths:\n",
        "        img = load_img(path_image, target_size=img_shape[:2])\n",
        "\n",
        "        el = img_to_array(img) / 255.0\n",
        "\n",
        "        k_train.append(el)\n",
        "\n",
        "    k_train = np.asarray(k_train)\n",
        "\n",
        "    return k_train\n",
        "\n",
        "\n",
        "def get_noise(n_sample=1, nlatent_dim=512):\n",
        "    noise = np.random.normal(0, 1, (n_sample, nlatent_dim))\n",
        "\n",
        "    return noise\n",
        "\n",
        "\n",
        "def plot_generated_images(noise, nsample=1, path_save=None, epoch=0, name=None):\n",
        "    imgs = decoder.predict(noise)\n",
        "    img = imgs[0]\n",
        "\n",
        "    # Solution: \"Clipping input data to the valid range for imshow\n",
        "    # with RGB data ([0..1] for floats or [0..255] for integers).\"\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    fig = plt.figure(figsize=(40, 10))\n",
        "    fig.patch.set_visible(False)\n",
        "    epoch_str = str(epoch)\n",
        "    extension = \".png\"\n",
        "    if name != None:\n",
        "        path_name = path_save + \"/\" + name + \"_\" + epoch_str + extension\n",
        "    else:\n",
        "        path_name = path_save + \"/\" + epoch_str + extension\n",
        "\n",
        "    ax = fig.add_subplot(1, nsample, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # fig.suptitle(\"Epoch:\" + epoch_str, fontsize=30)\n",
        "\n",
        "    plt.savefig(path_name,\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def build_encoder(img_shape):\n",
        "    input_img = Input(shape=img_shape, name=\"input_encoder\")\n",
        "    n_filter = 32\n",
        "\n",
        "    # Encoding\n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(input_img)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "\n",
        "    x = Conv2D(n_filter * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "\n",
        "    x = Conv2D(n_filter * 8, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = MaxPooling2D(padding='same')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    logsigma = Dense(512, activation=\"tanh\")(x)\n",
        "\n",
        "    model = Model(inputs=input_img, outputs=logsigma)\n",
        "    # model.summary()\n",
        "    return (model)\n",
        "\n",
        "\n",
        "def build_decoder_gen(en_shape=(512,)):\n",
        "    # impostazione del generatore\n",
        "    en_img = Input(shape=en_shape, name=\"input_decoder\")\n",
        "    img_dim = 32 * 16\n",
        "    n_filter = 32\n",
        "\n",
        "    x = Dense(img_dim * 8 * 8)(en_img)\n",
        "    x = Reshape((8, 8, img_dim))(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    # Decoding\n",
        "    x = Conv2D(n_filter * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = UpSampling2D()(x)\n",
        "\n",
        "    x = Conv2D(n_filter * 2, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = UpSampling2D()(x)\n",
        "\n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = UpSampling2D()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    model = Model(inputs=en_img, outputs=x)\n",
        "    return (model)\n",
        "\n",
        "\n",
        "def build_discriminator(img_shape):\n",
        "    input_img = Input(shape=img_shape, name=\"input_discriminator\")\n",
        "    n_filter = 32\n",
        "\n",
        "    x = Conv2D(n_filter, kernel_size=(3, 3), padding='same', strides=2)(input_img)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(n_filter * 2, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(n_filter * 4, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=(3, 3), padding='same', strides=2)(x)\n",
        "\n",
        "    x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=input_img, outputs=out)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(encoder, decoder, discriminator, gan, x_train,\n",
        "          dir_result=\"./\", epochs=10000, batch_size=512, checkpoint=10):\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        start_epoch = time.time()\n",
        "\n",
        "        np.random.shuffle(x_train)\n",
        "\n",
        "        batches_index = [x for x in range(0, len(x_train), batch_size)]\n",
        "\n",
        "        for i in range(0, len(batches_index)):\n",
        "\n",
        "            start = batches_index[i]\n",
        "\n",
        "            if i == len(batches_index) - 1:\n",
        "                end = len(x_train)\n",
        "            else:\n",
        "                end = batches_index[i + 1]\n",
        "\n",
        "            x_batch = x_train[start:end]\n",
        "            x_batch_size = len(x_batch)\n",
        "\n",
        "            matrix_true = np.ones((x_batch_size, 1))\n",
        "            matrix_false = np.zeros((x_batch_size, 1))\n",
        "\n",
        "            # Prediction\n",
        "            gen_img_enc = encoder.predict(x_batch)\n",
        "            gen_img_dec = decoder.predict(gen_img_enc)\n",
        "\n",
        "            noise = get_noise(n_sample=x_batch_size)\n",
        "            fake_img = decoder.predict(noise)\n",
        "\n",
        "            # Training\n",
        "\n",
        "            dsc_real = discriminator.train_on_batch(x_batch, matrix_true)\n",
        "            dsc_true_gen = discriminator.train_on_batch(gen_img_dec, matrix_true)\n",
        "            dsc_fake = discriminator.train_on_batch(fake_img, matrix_false)\n",
        "\n",
        "            dsc_true = 0.5*(dsc_real[0] + dsc_true_gen[0])\n",
        "\n",
        "            decoder_loss = decoder.train_on_batch(noise, x_batch)\n",
        "\n",
        "            gan_loss = gan.train_on_batch(x_batch, matrix_true)\n",
        "\n",
        "        if (epoch + 1) % checkpoint == 0:\n",
        "            history.append({\"Epoch:\": epoch + 1,\n",
        "                            \"Discriminator_loss_true:\": dsc_true,\n",
        "                            \"Discriminator_loss_fake:\": dsc_fake[0],\n",
        "                            \"Discriminator_acc_fake:\": dsc_fake[1],\n",
        "                            \"Decoder_loss:\": decoder_loss,\n",
        "                            \"GAN_loss:\": gan_loss\n",
        "                            })\n",
        "\n",
        "            noise = get_noise(n_sample=1)\n",
        "            plot_generated_images(noise, 1, dir_result, epoch + 1)\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        end_epoch = end_epoch - start_epoch\n",
        "        print(\"Ended Epoch {:0.0f}/{:1.0f} in: {:2.4f} s\".format(epoch + 1, epochs, end_epoch))\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "img_shape = (64, 64, 3)\n",
        "noise_shape = (512,)\n",
        "optimizer_std = SGD(0.0003)\n",
        "optimizer_disc = SGD(0.0004)\n",
        "\n",
        "pre_trained = False\n",
        "path_load_models = \"result/\"\n",
        "\n",
        "if pre_trained is False:\n",
        "    encoder = build_encoder(img_shape)\n",
        "    decoder = build_decoder_gen(noise_shape)\n",
        "    discriminator = build_discriminator(img_shape)\n",
        "\n",
        "    encoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "    decoder.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "\n",
        "    discriminator.compile(loss=\"mse\", optimizer=optimizer_disc,\n",
        "                          metrics=[\"accuracy\"])\n",
        "\n",
        "    img = Input(shape=img_shape, dtype=\"float32\")\n",
        "    encoded_img = encoder(img)\n",
        "    decoded_img = decoder(encoded_img)\n",
        "    val = discriminator(decoded_img)\n",
        "\n",
        "    gan = Model(inputs=[img], outputs=[val])\n",
        "    gan.compile(loss=\"mse\", optimizer=optimizer_std)\n",
        "\n",
        "else:\n",
        "    encoder = load_model(path_load_models + \"encoder.h5\")\n",
        "    decoder = load_model(path_load_models + \"decoder.h5\")\n",
        "    discriminator = load_model(path_load_models + \"discriminator.h5\")\n",
        "    gan = load_model(path_load_models + \"gan.h5\")\n",
        "\n",
        "x_train = get_dataset(\"./Linnaeus 5 64X64/train/flower\", img_shape)\n",
        "\n",
        "path = \"result/\"\n",
        "n_epoch = 1000\n",
        "n_batch = 64\n",
        "with tf.device(\"/gpu:0\"):\n",
        "    history = train(encoder, decoder, discriminator, gan, x_train,\n",
        "                    dir_result=path, epochs=n_epoch, batch_size=n_batch,\n",
        "                    checkpoint=40)\n",
        "\n",
        "with open(path + \"log_gan.txt\", \"w\") as f:\n",
        "    f.write(\"Epoch:\" + str(n_epoch) + \", Batch Size:\" + str(n_batch) + \"\\n\")\n",
        "    for item in history:\n",
        "        item_str = str(item) + \"\\n\"\n",
        "        f.write(item_str)\n",
        "\n",
        "# test post training\n",
        "for i in range(0, 5):\n",
        "    noise = get_noise(n_sample=1)\n",
        "\n",
        "    plot_generated_images(noise, 1, path, i, name=\"post_training\")\n",
        "\n",
        "encoder.save(path+\"encoder.h5\", save_format=\"tf\")\n",
        "decoder.save(path + \"decoder.h5\", save_format=\"tf\")\n",
        "discriminator.save(path + \"discriminator.h5\", save_format=\"tf\")\n",
        "gan.save(path + \"gan.h5\", save_format=\"tf\")\n",
        "\n",
        "end = time.time()\n",
        "end = end - start\n",
        "print(\"Finished in: \" + str(end) + \" s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWlMvZnpMi-Y"
      },
      "source": [
        "!rm -r ./result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCPFjesWPBAa"
      },
      "source": [
        "!rm -r ./Linnaeus\\ 5\\ 64X64"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}